"""
Benchmark impact of parameter changes on speed and accuracy of nextclade
Run as:
```bash
snakemake
```
"""


wildcard_constraints:
    dataset_name="[^/]+",
    build_type="[^/\._]+",
    build_type1="[^/\._]+",
    build_type2="[^/\._]+",
    seq_type="[^/]+",


# List of datasets to run on (TODO: Get from dataset list)
datasets = [
    "sars-cov-2",
    # "sars-cov-2-21L",
    # "MPXV",
    # "hMPXV",
    # "hMPXV_B1",
    # "flu_h1n1pdm_ha",
    # "flu_h1n1pdm_na",
    # "flu_h3n2_ha",
    # "flu_h3n2_na",
    # "flu_vic_ha",
    # "flu_vic_na",
    # "flu_yam_ha",
    # "rsv_a",
    # "rsv_b",
    # "sars-cov-2-no-recomb",
]

# Default outputs to produce
outputs = [
    "custom_diff_alignment_score.tsv",
    "run_time.csv",
]


rule all:
    input:
        expand(
            "results/{dataset_name}/{outputs}", dataset_name=datasets, outputs=outputs
        ),


rule build_eval:
    """
    Build (or recycle) eval branch binary
    """
    output:
        bin="bin/nextclade",
    shell:
        """
        cargo build --quiet --release --bin nextclade
        # Need to move up to cargo project root to find nextclade binary
        # There's probably a better way to do this
        cp "../../target/release/nextclade" {output.bin}
        """


rule download_dataset:
    input:
        bin="bin/nextclade",
    output:
        sequences="sequences/{dataset_name}/sequences.fasta",
        zip="dataset/{dataset_name}.zip",
    params:
        dir="dataset/{dataset_name}",
    shell:
        """
        ./{input.bin} dataset get -n {wildcards.dataset_name} --output-zip {output.zip}
        ./{input.bin} dataset get -n {wildcards.dataset_name} --output-dir {params.dir}
        cp {params.dir}/sequences.fasta {output.sequences}
        """


rule run_nextclade:
    input:
        bin="bin/nextclade",
        sequences="sequences/{dataset_name}",
        dataset="dataset/{dataset_name}.zip",
    output:
        tsv="results/{dataset_name}/exc_{excess}_term_{term}_retry_{retry}_mis_{mis}_wind_{wind}_minml_{minml}_minseedcover_{minseedcover}/nextclade.tsv",
        runtime="results/{dataset_name}/exc_{excess}_term_{term}_retry_{retry}_mis_{mis}_wind_{wind}_minml_{minml}_minseedcover_{minseedcover}/runtime.csv",
    threads: 4
    shell:
        """
        hyperfine --runs 1 \
        --command-name "{wildcards.dataset_name};exc_{wildcards.excess};term_{wildcards.term};retry_{wildcards.retry};mis_{wildcards.mis};wind_{wildcards.wind};minml_{wildcards.minml};minseedcover_{wildcards.minseedcover}" \
        --export-csv {output.runtime} \
        './{input.bin} run -j {threads} \
        --input-dataset {input.dataset} \
        --output-tsv {output.tsv} \
        --excess-bandwidth {wildcards.excess} \
        --terminal-bandwidth {wildcards.term} \
        --max-alignment-attempts {wildcards.retry} \
        --gap-alignment-side right \
        --allowed-mismatches {wildcards.mis} \
        --window-size {wildcards.wind} \
        --min-match-length {wildcards.minml} \
        --min-seed-cover {wildcards.minseedcover} \
        --without-greedy-tree-builder \
        {input.sequences}/*'
        """


# Uses nextclade binary in PATH
rule run_nextclade_old:
    input:
        sequences="sequences/{dataset_name}",
        dataset="dataset/{dataset_name}.zip",
    output:
        tsv="results/{dataset_name}/old/nextclade.tsv",
        runtime="results/{dataset_name}/old/runtime.csv",
    threads: 4
    shell:
        """
        hyperfine \
        --runs 1 \
        --command-name "old" \
        --export-csv {output.runtime} \
            'nextclade run -j {threads} \
            --input-dataset {input.dataset} \
            --output-tsv {output.tsv} \
            --gap-alignment-side right \
            {input.sequences}/*'
        """


rule tsv_column_diff:
    input:
        tsv1="results/{dataset_name}/{para1}/nextclade.tsv",
        tsv2="results/{dataset_name}/{para2}/nextclade.tsv",
    output:
        diff="results/{dataset_name}/{para1}:{para2}/diff_columns.tsv",
    shell:
        """
        python3 scripts/tsv-diff.py \
        --base-tsv-path {input.tsv1} \
        --eval-tsv-path {input.tsv2} \
        > {output.diff}
        """


rule tsv_column_diff_old:
    input:
        tsv1="results/{dataset_name}/old/nextclade.tsv",
        tsv2="results/{dataset_name}/{para}/nextclade.tsv",
    output:
        diff="results/{dataset_name}/{para}/diff_columns.tsv",
    shell:
        """
        # Add --score-correct if N penalty active
        python3 scripts/tsv-diff.py \
        --base-tsv-path {input.tsv1} \
        --eval-tsv-path {input.tsv2} \
        > {output.diff}
        """


# TODO: Deal with newly aligning sequences, present in one, not the other
rule compare_vs_old:
    input:
        oldtsv="results/{dataset_name}/old/nextclade.tsv",
        oldruntime="results/{dataset_name}/old/runtime.csv",
        newtsv="results/{dataset_name}/{para}/nextclade.tsv",
        newruntime="results/{dataset_name}/{para}/runtime.csv",
    output:
        diff="results/{dataset_name}/{para}/new_vs_old.tsv",
    shell:
        """
        python3 scripts/new-old-diff.py \
        --old-tsv-path {input.oldtsv} \
        --old-runtime {input.oldruntime} \
        --new-tsv-path {input.newtsv} \
        --new-runtime {input.newruntime} \
        --params {wildcards.para} \
        > {output.diff}
        """


# rule alignment_score_diff:
#     input:
#         tsv1="results/{dataset_name}/base/{seq_type}.tsv",
#         tsv2="results/{dataset_name}/eval/{seq_type}.tsv",
#     output:
#         diff="results/{dataset_name}/{seq_type}_diff_alignment_score.tsv",
#     shell:
#         """
#         python3 scripts/tsv-diff.py \
#         --base-tsv-path {input.tsv1} \
#         --eval-tsv-path {input.tsv2} \
#         --columns alignmentScore \
#         > {output.diff}
#         """


rule join_summary:
    output:
        summary="results/{dataset_name}/new_vs_old.tsv",
    params:
        new="results/{dataset_name}/*/new_vs_old.tsv",
    shell:
        """
        tsv-append -H {params.new} >{output.summary}
        """


default_params = {
    "exc": 9,
    "term": 50,
    "retry": 3,
    "mis": 4,
    "wind": 30,
    "minml": 40,
    "minseedcover": 0.33,
}


def para_sweep(param, values):
    sweep = []
    for value in values:
        default = default_params.copy()
        default[param] = value
        sweep.append(default)
    return sweep


para = []
para.extend(para_sweep("exc", range(1, 40, 5)))
para.extend(para_sweep("term", range(1, 400, 20)))
para.extend(para_sweep("retry", [0, 1, 2, 3]))
para.extend(para_sweep("mis", range(1, 10, 1)))
para.extend(para_sweep("wind", range(15, 100, 5)))
para.extend(para_sweep("minseedcover", [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5]))


rule sc2:
    input:
        [
            expand(
                "results/sars-cov-2/exc_{exc}_term_{term}_retry_{retry}_mis_{mis}_wind_{wind}_minml_{minml}_minseedcover_{minseedcover}/new_vs_old.tsv",
                **params
            )
            for params in para
        ],
    output:
        "results/sars-cov-2/sweep/new_vs_old.tsv",
    shell:
        """
        tsv-append -H {input} >{output}
        """

rule mpxv: 
    input:
        [
            expand(
                "results/MPXV/exc_{exc}_term_{term}_retry_{retry}_mis_{mis}_wind_{wind}_minml_{minml}_minseedcover_{minseedcover}/new_vs_old.tsv",
                **params
            )
            for params in para
        ],
    output:
        "results/MPXV/sweep/new_vs_old.tsv",
    shell:
        """
        tsv-append -H {input} >{output}
        """

rule clean:
    """
    Remove results and eval directories
    """
    shell:
        """
        rm -rf results eval_build
        """


rule clobber:
    """
    Remove dataset, results, base, and eval directories
    """
    shell:
        """
        rm -rf results eval_build base_build dataset
        """
